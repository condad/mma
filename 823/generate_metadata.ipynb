{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0662b76",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Analysis and Currency Market Correlation\n",
    "\n",
    "This notebook analyzes the correlation between Trump's social media sentiment and currency market movements by:\n",
    "1. Loading tweet data\n",
    "2. Extracting country mentions and mapping to currencies\n",
    "3. Analyzing sentiment using NLTK\n",
    "4. Correlating with exchange rate movements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aec287",
   "metadata": {},
   "source": [
    "## 1. Loading Data\n",
    "\n",
    "Import required libraries and load the tweets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827a295d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/condad/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pycountry\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.data.find('vader_lexicon')\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "094ddc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6896 tweets\n",
      "Date range: 2024-10-13 04:23:37+00:00 to 2025-10-25 22:15:50+00:00\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6896 entries, 0 to 6895\n",
      "Data columns (total 25 columns):\n",
      " #   Column                 Non-Null Count  Dtype              \n",
      "---  ------                 --------------  -----              \n",
      " 0   page_number            6896 non-null   int64              \n",
      " 1   browse_flag            6896 non-null   bool               \n",
      " 2   date                   6896 non-null   datetime64[ns, UTC]\n",
      " 3   document_id            6896 non-null   int64              \n",
      " 4   image_url              6896 non-null   object             \n",
      " 5   media_type             6896 non-null   object             \n",
      " 6   sequence               6896 non-null   int64              \n",
      " 7   speaker                6896 non-null   object             \n",
      " 8   speaker_id             6896 non-null   object             \n",
      " 9   subject                6896 non-null   object             \n",
      " 10  text                   6896 non-null   object             \n",
      " 11  type                   6863 non-null   object             \n",
      " 12  word_count             6896 non-null   object             \n",
      " 13  deleted_flag           6879 non-null   object             \n",
      " 14  account_url            6555 non-null   object             \n",
      " 15  handle                 6869 non-null   object             \n",
      " 16  id                     6881 non-null   object             \n",
      " 17  platform               6886 non-null   object             \n",
      " 18  post_url               6894 non-null   object             \n",
      " 19  social_author          6893 non-null   object             \n",
      " 20  social_favorite_count  6891 non-null   object             \n",
      " 21  social_repost_count    6553 non-null   object             \n",
      " 22  social_visibility      6537 non-null   object             \n",
      " 23  search_id              6869 non-null   object             \n",
      " 24  score                  27 non-null     object             \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), int64(3), object(20)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "\n",
      "First few tweets:\n",
      "                       date                                               text\n",
      "0 2025-10-25 22:15:50+00:00  I am on my way to Malaysia, where I will sign ...\n",
      "1 2025-10-25 21:43:11+00:00  RT @realDonaldTrump Canada was caught, red han...\n",
      "2 2025-10-25 21:39:14+00:00  https://www. dailysignal.com/2025/10/22/tru mp...\n",
      "3 2025-10-25 21:38:57+00:00  https://www. foxnews.com/opinion/new-high-t ec...\n",
      "4 2025-10-25 21:38:39+00:00  https://www. foxnews.com/politics/scoop-tru mp...\n"
     ]
    }
   ],
   "source": [
    "# Load the tweets dataset\n",
    "df = pd.read_csv('tweets.csv', on_bad_lines='skip')\n",
    "\n",
    "# Convert date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "print(f\"Loaded {len(df)} tweets\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few tweets:\")\n",
    "print(df[['date', 'text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f96ed5",
   "metadata": {},
   "source": [
    "## 2. Adding Country and Currency Columns\n",
    "\n",
    "Extract country mentions from tweets and map them to their respective currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12f90ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1949 tweets with country mentions out of 6896 total tweets\n",
      "Percentage with countries: 28.3%\n",
      "\n",
      "Top 10 mentioned countries:\n",
      "United States     1751\n",
      "Russia             128\n",
      "China               98\n",
      "Canada              55\n",
      "Mexico              44\n",
      "United Kingdom      33\n",
      "India               26\n",
      "Japan               21\n",
      "France              12\n",
      "South Korea         10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def find_countries_regex(text):\n",
    "    \"\"\"Extract country names from text using regex patterns\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    # Common country name patterns\n",
    "    country_patterns = [\n",
    "        r'\\b(?:United States|USA|US|America|American)\\b',\n",
    "        r'\\bCanada\\b|\\bCanadian\\b',\n",
    "        r'\\bChina\\b|\\bChinese\\b',\n",
    "        r'\\bJapan\\b|\\bJapanese\\b',\n",
    "        r'\\b(?:United Kingdom|UK|Britain|British|England|English)\\b',\n",
    "        r'\\bGermany\\b|\\bGerman\\b',\n",
    "        r'\\bFrance\\b|\\bFrench\\b',\n",
    "        r'\\bItaly\\b|\\bItalian\\b',\n",
    "        r'\\bSpain\\b|\\bSpanish\\b',\n",
    "        r'\\bIndia\\b|\\bIndian\\b',\n",
    "        r'\\bBrazil\\b|\\bBrazilian\\b',\n",
    "        r'\\bMexico\\b|\\bMexican\\b',\n",
    "        r'\\bRussia\\b|\\bRussian\\b',\n",
    "        r'\\bSouth Korea\\b|\\bKorea\\b|\\bKorean\\b',\n",
    "        r'\\bAustralia\\b|\\bAustralian\\b',\n",
    "        r'\\bSwitzerland\\b|\\bSwiss\\b',\n",
    "        r'\\bNorway\\b|\\bNorwegian\\b',\n",
    "        r'\\bSweden\\b|\\bSwedish\\b',\n",
    "        r'\\bDenmark\\b|\\bDanish\\b',\n",
    "        r'\\bNetherlands\\b|\\bDutch\\b'\n",
    "    ]\n",
    "\n",
    "    countries = set()\n",
    "    text_upper = text.upper()\n",
    "\n",
    "    for pattern in country_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            # Normalize country names\n",
    "            if re.match(r'(?i)united states|usa|us|america|american', match):\n",
    "                countries.add('United States')\n",
    "            elif re.match(r'(?i)canada|canadian', match):\n",
    "                countries.add('Canada')\n",
    "            elif re.match(r'(?i)china|chinese', match):\n",
    "                countries.add('China')\n",
    "            elif re.match(r'(?i)japan|japanese', match):\n",
    "                countries.add('Japan')\n",
    "            elif re.match(r'(?i)united kingdom|uk|britain|british|england|english', match):\n",
    "                countries.add('United Kingdom')\n",
    "            elif re.match(r'(?i)germany|german', match):\n",
    "                countries.add('Germany')\n",
    "            elif re.match(r'(?i)france|french', match):\n",
    "                countries.add('France')\n",
    "            elif re.match(r'(?i)italy|italian', match):\n",
    "                countries.add('Italy')\n",
    "            elif re.match(r'(?i)spain|spanish', match):\n",
    "                countries.add('Spain')\n",
    "            elif re.match(r'(?i)india|indian', match):\n",
    "                countries.add('India')\n",
    "            elif re.match(r'(?i)brazil|brazilian', match):\n",
    "                countries.add('Brazil')\n",
    "            elif re.match(r'(?i)mexico|mexican', match):\n",
    "                countries.add('Mexico')\n",
    "            elif re.match(r'(?i)russia|russian', match):\n",
    "                countries.add('Russia')\n",
    "            elif re.match(r'(?i)south korea|korea|korean', match):\n",
    "                countries.add('South Korea')\n",
    "            elif re.match(r'(?i)australia|australian', match):\n",
    "                countries.add('Australia')\n",
    "            elif re.match(r'(?i)switzerland|swiss', match):\n",
    "                countries.add('Switzerland')\n",
    "            elif re.match(r'(?i)norway|norwegian', match):\n",
    "                countries.add('Norway')\n",
    "            elif re.match(r'(?i)sweden|swedish', match):\n",
    "                countries.add('Sweden')\n",
    "            elif re.match(r'(?i)denmark|danish', match):\n",
    "                countries.add('Denmark')\n",
    "            elif re.match(r'(?i)netherlands|dutch', match):\n",
    "                countries.add('Netherlands')\n",
    "\n",
    "    return list(countries)\n",
    "\n",
    "# Extract countries from tweets\n",
    "df['countries_found'] = df['text'].apply(find_countries_regex)\n",
    "df['countries_mentioned'] = df['countries_found'].apply(lambda x: ', '.join(x) if x else '')\n",
    "\n",
    "# Filter tweets with country mentions\n",
    "tweets_with_countries = df[df['countries_mentioned'] != ''].copy()\n",
    "\n",
    "print(f\"Found {len(tweets_with_countries)} tweets with country mentions out of {len(df)} total tweets\")\n",
    "print(f\"Percentage with countries: {len(tweets_with_countries)/len(df)*100:.1f}%\")\n",
    "\n",
    "# Show country mention frequency\n",
    "all_countries = []\n",
    "for countries in tweets_with_countries['countries_found']:\n",
    "    all_countries.extend(countries)\n",
    "\n",
    "country_counts = pd.Series(all_countries).value_counts()\n",
    "print(\"\\nTop 10 mentioned countries:\")\n",
    "print(country_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "939a6d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully mapped 1949 tweets to currencies\n",
      "\n",
      "Currency mention frequency:\n",
      "USD    1751\n",
      "RUB     128\n",
      "CNY      98\n",
      "CAD      55\n",
      "MXN      44\n",
      "EUR      33\n",
      "GBP      33\n",
      "INR      26\n",
      "JPY      21\n",
      "KRW      10\n",
      "BRL       5\n",
      "AUD       4\n",
      "SEK       3\n",
      "CHF       2\n",
      "NOK       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def country_to_currency(country_name):\n",
    "    \"\"\"Map country names to their currency codes\"\"\"\n",
    "    # Manual mapping for common countries\n",
    "    country_currency_map = {\n",
    "        'United States': 'USD',\n",
    "        'Canada': 'CAD',\n",
    "        'United Kingdom': 'GBP',\n",
    "        'Japan': 'JPY',\n",
    "        'Germany': 'EUR',\n",
    "        'France': 'EUR',\n",
    "        'Italy': 'EUR',\n",
    "        'Spain': 'EUR',\n",
    "        'Netherlands': 'EUR',\n",
    "        'China': 'CNY',\n",
    "        'India': 'INR',\n",
    "        'Brazil': 'BRL',\n",
    "        'Mexico': 'MXN',\n",
    "        'Russia': 'RUB',\n",
    "        'South Korea': 'KRW',\n",
    "        'Australia': 'AUD',\n",
    "        'Switzerland': 'CHF',\n",
    "        'Norway': 'NOK',\n",
    "        'Sweden': 'SEK',\n",
    "        'Denmark': 'DKK'\n",
    "    }\n",
    "\n",
    "    if country_name in country_currency_map:\n",
    "        return country_currency_map[country_name]\n",
    "\n",
    "    # Try pycountry for other countries\n",
    "    try:\n",
    "        country = pycountry.countries.search_fuzzy(country_name)[0]\n",
    "        currency = pycountry.currencies.get(numeric=country.numeric)\n",
    "        if currency:\n",
    "            return currency.alpha_3\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "# Map countries to currencies\n",
    "tweets_with_countries['currencies'] = tweets_with_countries['countries_found'].apply(\n",
    "    lambda countries: [country_to_currency(country) for country in countries if country_to_currency(country)]\n",
    ")\n",
    "\n",
    "tweets_with_countries['currency_codes'] = tweets_with_countries['currencies'].apply(\n",
    "    lambda x: ', '.join(set(x)) if x else ''\n",
    ")\n",
    "\n",
    "# Add currencies_found to the main dataframe\n",
    "df['currencies_found'] = df['countries_found'].apply(\n",
    "    lambda countries: [country_to_currency(country) for country in countries if country_to_currency(country)]\n",
    ")\n",
    "\n",
    "# Add currency_codes to the main dataframe\n",
    "df['currency_codes'] = df['currencies_found'].apply(\n",
    "    lambda x: ', '.join(set(x)) if x else ''\n",
    ")\n",
    "\n",
    "# Filter tweets with valid currency mappings\n",
    "tweets_with_currencies = tweets_with_countries[tweets_with_countries['currency_codes'] != ''].copy()\n",
    "\n",
    "print(f\"Successfully mapped {len(tweets_with_currencies)} tweets to currencies\")\n",
    "\n",
    "# Show currency mention frequency\n",
    "all_currencies = []\n",
    "for currencies in tweets_with_currencies['currencies']:\n",
    "    all_currencies.extend(currencies)\n",
    "\n",
    "currency_counts = pd.Series(all_currencies).value_counts()\n",
    "print(\"\\nCurrency mention frequency:\")\n",
    "print(currency_counts.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd54b1f",
   "metadata": {},
   "source": [
    "## 3. Adding Sentiment Columns\n",
    "\n",
    "Analyze the sentiment of each tweet using NLTK's VADER sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e0e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment for all tweets...\n",
      "Sentiment analysis complete!\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment_label\n",
      "neutral     3507\n",
      "positive    2430\n",
      "negative     959\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average compound sentiment: 0.177\n",
      "Sentiment range: -0.998 to 0.997\n",
      "\n",
      "Sample tweets with sentiment scores:\n",
      "\n",
      "Tweet: I am on my way to Malaysia, where I will sign the great Peace Deal, which I proudly brokered between...\n",
      "Compound Score: 0.968 (positive)\n",
      "\n",
      "Tweet: RT @realDonaldTrump Canada was caught, red handed, putting up a fraudulent advertisement on Ronald R...\n",
      "Compound Score: -0.893 (negative)\n",
      "\n",
      "Tweet: https://www. dailysignal.com/2025/10/22/tru mps-middle-east-triumph-embarrassed-self-proclaimed-expe...\n",
      "Compound Score: 0.000 (neutral)\n",
      "\n",
      "Tweet: https://www. foxnews.com/opinion/new-high-t ech-tool-trump-using-secure-our-border...\n",
      "Compound Score: 0.000 (neutral)\n",
      "\n",
      "Tweet: https://www. foxnews.com/politics/scoop-tru mps-memphis-crime-crackdown-locates-dozens-missing-kids-...\n",
      "Compound Score: 0.000 (neutral)\n",
      "\n",
      "Tweet: https:// dailycaller.com/2025/10/22/jef f-merkley-floor-speech-government-shutdown/...\n",
      "Compound Score: 0.000 (neutral)\n",
      "\n",
      "Tweet: https://www. foxnews.com/us/new-york-city-i ce-raid-nets-9-arrests-illegal-aliens-from-west-africa-4...\n",
      "Compound Score: 0.000 (neutral)\n",
      "\n",
      "Tweet: https:// justthenews.com/government/sec urity/deadly-serious-trump-counterterrorism-program-kills-37...\n",
      "Compound Score: 0.000 (neutral)\n",
      "\n",
      "Tweet: Canada was caught, red handed, putting up a fraudulent advertisement on Ronald Reaganâ€™s Speech on Ta...\n",
      "Compound Score: -0.893 (negative)\n",
      "\n",
      "Tweet: We have a very strong PEACE in the Middle East, and I believe it has a good chance of being EVERLAST...\n",
      "Compound Score: 0.910 (positive)\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment_scores(text):\n",
    "    \"\"\"Get sentiment scores for a given text using VADER\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    if pd.isna(text):\n",
    "        return {\n",
    "            'sentiment_compound': 0.0,\n",
    "            'sentiment_positive': 0.0,\n",
    "            'sentiment_negative': 0.0,\n",
    "            'sentiment_neutral': 0.0\n",
    "        }\n",
    "\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return {\n",
    "        'sentiment_compound': scores['compound'],  # Overall sentiment (-1 to 1)\n",
    "        'sentiment_positive': scores['pos'],       # Positive sentiment ratio\n",
    "        'sentiment_negative': scores['neg'],       # Negative sentiment ratio\n",
    "        'sentiment_neutral': scores['neu']         # Neutral sentiment ratio\n",
    "    }\n",
    "\n",
    "def categorize_sentiment(compound_score):\n",
    "    \"\"\"Categorize sentiment based on compound score\"\"\"\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Apply sentiment analysis to all tweets\n",
    "print(\"Analyzing sentiment for all tweets...\")\n",
    "sentiment_results = df['text'].apply(get_sentiment_scores)\n",
    "\n",
    "# Extract sentiment scores into separate columns\n",
    "df['sentiment_compound'] = sentiment_results.apply(lambda x: x['sentiment_compound'])\n",
    "df['sentiment_positive'] = sentiment_results.apply(lambda x: x['sentiment_positive'])\n",
    "df['sentiment_negative'] = sentiment_results.apply(lambda x: x['sentiment_negative'])\n",
    "df['sentiment_neutral'] = sentiment_results.apply(lambda x: x['sentiment_neutral'])\n",
    "\n",
    "# Add categorical sentiment label\n",
    "df['sentiment_label'] = df['sentiment_compound'].apply(categorize_sentiment)\n",
    "\n",
    "print(\"Sentiment analysis complete!\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "print(f\"\\nAverage compound sentiment: {df['sentiment_compound'].mean():.3f}\")\n",
    "print(f\"Sentiment range: {df['sentiment_compound'].min():.3f} to {df['sentiment_compound'].max():.3f}\")\n",
    "\n",
    "# Show sample tweets with sentiment\n",
    "print(\"\\nSample tweets with sentiment scores:\")\n",
    "sample_df = df[['text', 'sentiment_compound', 'sentiment_label']].head(10)\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"\\nTweet: {row['text'][:100]}...\")\n",
    "    print(f\"Compound Score: {row['sentiment_compound']:.3f} ({row['sentiment_label']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b25b16",
   "metadata": {},
   "source": [
    "## 4. Save Metadata\n",
    "\n",
    "Output file is called 'tweet_metadata.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1147b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce and save only the requested columns to avoid duplicated/unnecessary data\n",
    "print(\"Current column order:\")\n",
    "print(list(df.columns))\n",
    "\n",
    "# Determine ID column (use 'id' if present, else add 'tweet_id' from the index)\n",
    "if 'id' in df.columns:\n",
    "    id_col = 'id'\n",
    "else:\n",
    "    if 'tweet_id' not in df.columns:\n",
    "        df = df.reset_index().rename(columns={'index': 'tweet_id'})\n",
    "    id_col = 'tweet_id'\n",
    "\n",
    "# Columns to keep\n",
    "desired_cols = [\n",
    "    id_col,\n",
    "    'countries_found',\n",
    "    'currencies_found',\n",
    "    'sentiment_compound',\n",
    "    'sentiment_positive',\n",
    "    'sentiment_negative',\n",
    "    'sentiment_neutral',\n",
    "    'sentiment_label'\n",
    "]\n",
    "\n",
    "# Only keep columns that actually exist in the dataframe\n",
    "present_cols = [c for c in desired_cols if c in df.columns]\n",
    "\n",
    "df_reduced = df[present_cols].copy()\n",
    "\n",
    "print(f\"\\nSaving reduced dataframe with columns: {present_cols}\")\n",
    "output_filename = 'tweet_metadata.csv'\n",
    "df_reduced.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Saved reduced dataframe to {output_filename}\")\n",
    "print(f\"Shape: {df_reduced.shape}\")\n",
    "print(f\"\\nFirst few rows of reduced data:\")\n",
    "print(df_reduced.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
