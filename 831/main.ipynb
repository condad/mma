{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599c327a",
   "metadata": {},
   "source": [
    "# Data Analysis and Logistic Regression\n",
    "\n",
    "This notebook loads data directly from CSV files and performs logistic regression to predict customer response behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8fb4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded contact history data: (1527978, 9)\n",
      "Loaded customer data: (190339, 7)\n",
      "Loaded customer score data: (1047507, 25)\n",
      "Loaded campaign data: (55, 7)\n",
      "Loaded cell data: (17249, 11)\n",
      "Loaded offer data: (19699, 5)\n",
      "Loaded customer score data: (1047507, 25)\n",
      "Loaded campaign data: (55, 7)\n",
      "Loaded cell data: (17249, 11)\n",
      "Loaded offer data: (19699, 5)\n",
      "Loaded customer fact data: (1047507, 39)\n",
      "\n",
      "Contact History Data Columns:\n",
      "['date_key', 'customer_key', 'customer_score_key', 'ch_campaign_key', 'ch_cell_key', 'ch_offer_key', 'cch_responder_key', 'offer_amount_loc_currency', 'volume']\n",
      "\n",
      "Columns ending with 'key': ['date_key', 'customer_key', 'customer_score_key', 'ch_campaign_key', 'ch_cell_key', 'ch_offer_key', 'cch_responder_key']\n",
      "\n",
      "First few rows of contact history data:\n",
      "              date_key  customer_key  customer_score_key  ch_campaign_key  \\\n",
      "0  2018-01-31 00:00:00       5973888           343229725             3671   \n",
      "1  2018-02-28 00:00:00       2458586           345294541             4115   \n",
      "2  2018-02-28 00:00:00       6365820           343707868             3674   \n",
      "3  2018-02-28 00:00:00       5959280           344161648             3674   \n",
      "4  2018-01-31 00:00:00       6568472           341757166             3676   \n",
      "\n",
      "   ch_cell_key  ch_offer_key  cch_responder_key  offer_amount_loc_currency  \\\n",
      "0       247330      14077090                  0                   147000.0   \n",
      "1       401529      16759104                  0                   420000.0   \n",
      "2       247513      14077491                  0                   116000.0   \n",
      "3       247519      14077575                  0                    76670.0   \n",
      "4       247458      14077302                  0                   181000.0   \n",
      "\n",
      "   volume  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n",
      "Loaded customer fact data: (1047507, 39)\n",
      "\n",
      "Contact History Data Columns:\n",
      "['date_key', 'customer_key', 'customer_score_key', 'ch_campaign_key', 'ch_cell_key', 'ch_offer_key', 'cch_responder_key', 'offer_amount_loc_currency', 'volume']\n",
      "\n",
      "Columns ending with 'key': ['date_key', 'customer_key', 'customer_score_key', 'ch_campaign_key', 'ch_cell_key', 'ch_offer_key', 'cch_responder_key']\n",
      "\n",
      "First few rows of contact history data:\n",
      "              date_key  customer_key  customer_score_key  ch_campaign_key  \\\n",
      "0  2018-01-31 00:00:00       5973888           343229725             3671   \n",
      "1  2018-02-28 00:00:00       2458586           345294541             4115   \n",
      "2  2018-02-28 00:00:00       6365820           343707868             3674   \n",
      "3  2018-02-28 00:00:00       5959280           344161648             3674   \n",
      "4  2018-01-31 00:00:00       6568472           341757166             3676   \n",
      "\n",
      "   ch_cell_key  ch_offer_key  cch_responder_key  offer_amount_loc_currency  \\\n",
      "0       247330      14077090                  0                   147000.0   \n",
      "1       401529      16759104                  0                   420000.0   \n",
      "2       247513      14077491                  0                   116000.0   \n",
      "3       247519      14077575                  0                    76670.0   \n",
      "4       247458      14077302                  0                   181000.0   \n",
      "\n",
      "   volume  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n"
     ]
    }
   ],
   "source": [
    "# Load all datasets directly from CSV files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load contact history fact data\n",
    "contact_history_raw = pd.read_csv('data/Contact_History_Fact_0.csv')\n",
    "print(f\"Loaded contact history data: {contact_history_raw.shape}\")\n",
    "\n",
    "# Load all related datasets\n",
    "customer_raw = pd.read_csv('data/Customer_0_RV.csv')\n",
    "print(f\"Loaded customer data: {customer_raw.shape}\")\n",
    "\n",
    "customer_score_raw = pd.read_csv('data/Customer_Score_0_RV.csv')\n",
    "print(f\"Loaded customer score data: {customer_score_raw.shape}\")\n",
    "\n",
    "campaign_raw = pd.read_csv('data/CH_Campaign_0_V.csv')\n",
    "print(f\"Loaded campaign data: {campaign_raw.shape}\")\n",
    "\n",
    "cell_raw = pd.read_csv('data/CH_Cell_0_V.csv')\n",
    "print(f\"Loaded cell data: {cell_raw.shape}\")\n",
    "\n",
    "offer_raw = pd.read_csv('data/CH_Offer_0_V.csv')\n",
    "print(f\"Loaded offer data: {offer_raw.shape}\")\n",
    "\n",
    "customer_fact_raw = pd.read_csv('data/Customer_Fact_0_V.csv')\n",
    "print(f\"Loaded customer fact data: {customer_fact_raw.shape}\")\n",
    "\n",
    "print(\"\\nContact History Data Columns:\")\n",
    "print(contact_history_raw.columns.tolist())\n",
    "\n",
    "# Find columns ending with \"key\"\n",
    "key_columns = [col for col in contact_history_raw.columns if col.endswith('key')]\n",
    "print(f\"\\nColumns ending with 'key': {key_columns}\")\n",
    "\n",
    "print(\"\\nFirst few rows of contact history data:\")\n",
    "print(contact_history_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2757bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets for merging:\n",
      "Customer data: (190339, 7)\n",
      "Customer score data: (1047507, 25)\n",
      "Campaign data: (55, 7)\n",
      "Cell data: (17249, 11)\n",
      "Offer data: (19699, 5)\n",
      "Customer fact data: (1047507, 39)\n",
      "\n",
      "Customer data columns: ['customer_key', 'MarriedIndicator', 'SinceDate', 'SinceDateBinned', 'HomeownerFlag', 'RetailFlag', 'OccupationCode']\n",
      "Customer score data columns: ['customer_score_key', 'TIBScore', 'FrequencyDecile', 'RecencyDecile', 'TotalIncomeDecile', 'TotalRetBalanceDecile', 'ATMChannelFlag', 'BranchChannelFlag', 'ITBChannelFlag', 'POSChannelFlag', 'TSChannelFlag', 'Age', 'NumProducts', 'ValidGenderFlag', 'OnlineRegFlag', 'OnlineActiveFlagW', 'OnlineActiveFlagM', 'num_atm_trans', 'num_branch_trans', 'num_online_trans', 'num_ivr_trans', 'num_pos_trans', 'PayrollFlag', 'num_ITB_bill_paymts_trans', 'num_offline_bill_pmt_tran']\n",
      "Campaign data columns: ['ch_campaign_key', 'campaign_code', 'campaign_start_date', 'campaign_expiry_date', 'campaign_id', 'campaign_label', 'campaign_type']\n",
      "Cell data columns: ['ch_cell_key', 'channel1', 'channel2', 'channel3', 'channel4', 'channel5', 'channel6', 'channel7', 'channel8', 'channel9', 'iso_code']\n",
      "Offer data columns: ['ch_offer_key', 'offer_condition', 'offer_product', 'offer_sub_product', 'offer_currency']\n",
      "Customer fact data columns: ['date_key', 'customer_key', 'customer_score_key', 'cciso_key', 'BalanceChequing', 'BalanceCC', 'BalanceResMortgages', 'BalanceRetLoans', 'BalanceSaving', 'BalanceScotialine', 'BalanceRetTD', 'NumberChequings', 'NumberCC', 'NumberResMortgages', 'NumberRetLoans', 'NumberSavings', 'NumberScotialines', 'NumberRetTD', 'TotalRetDebt', 'TotalRetDeposits', 'TotalNumberAccounts', 'TotalRetBalance', 'NumberTransactions', 'DerivedIncome', 'BalanceRetMF', 'NumberRetMF', 'NetRevenue', 'NetIncome', 'Balance_Ret_Autoloan', 'atm_transact_amt', 'branch_transact_amt', 'online_transact_amt', 'ivr_transact_amt', 'pos_transact_amt', 'online_payments_amt', 'online_bill_payments_amt', 'atm_deposits_amt', 'offline_bill_payments_amt', 'CC_CreditLimit']\n",
      "\n",
      "Starting with contact history data: (1527978, 9)\n",
      "After merging with customer data: (1527978, 15)\n",
      "After merging with customer score data: (1527978, 39)\n",
      "After merging with customer score data: (1527978, 39)\n",
      "After merging with campaign data: (1527978, 45)\n",
      "After merging with campaign data: (1527978, 45)\n",
      "After merging with cell data: (1527978, 55)\n",
      "After merging with cell data: (1527978, 55)\n",
      "After merging with offer data: (1527978, 59)\n",
      "\n",
      "Final merged dataset shape: (1527978, 59)\n",
      "Target variable (cch_responder_key) distribution:\n",
      "cch_responder_key\n",
      "0    1483051\n",
      "1      44927\n",
      "Name: count, dtype: int64\n",
      "After merging with offer data: (1527978, 59)\n",
      "\n",
      "Final merged dataset shape: (1527978, 59)\n",
      "Target variable (cch_responder_key) distribution:\n",
      "cch_responder_key\n",
      "0    1483051\n",
      "1      44927\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(\"Available datasets for merging:\")\n",
    "print(f\"Customer data: {customer_raw.shape}\")\n",
    "print(f\"Customer score data: {customer_score_raw.shape}\")\n",
    "print(f\"Campaign data: {campaign_raw.shape}\")\n",
    "print(f\"Cell data: {cell_raw.shape}\")\n",
    "print(f\"Offer data: {offer_raw.shape}\")\n",
    "print(f\"Customer fact data: {customer_fact_raw.shape}\")\n",
    "\n",
    "# Check the key columns in each dataset\n",
    "print(\"\\nCustomer data columns:\", customer_raw.columns.tolist())\n",
    "print(\"Customer score data columns:\", customer_score_raw.columns.tolist())\n",
    "print(\"Campaign data columns:\", campaign_raw.columns.tolist())\n",
    "print(\"Cell data columns:\", cell_raw.columns.tolist())\n",
    "print(\"Offer data columns:\", offer_raw.columns.tolist())\n",
    "print(\"Customer fact data columns:\", customer_fact_raw.columns.tolist())\n",
    "\n",
    "# Start with the contact history data\n",
    "merged_data = contact_history_raw.copy()\n",
    "print(f\"\\nStarting with contact history data: {merged_data.shape}\")\n",
    "\n",
    "# Merge with customer data on customer_key\n",
    "merged_data = merged_data.merge(customer_raw, on='customer_key', how='left')\n",
    "print(f\"After merging with customer data: {merged_data.shape}\")\n",
    "\n",
    "# Merge with customer score data on customer_score_key\n",
    "merged_data = merged_data.merge(customer_score_raw, on='customer_score_key', how='left')\n",
    "print(f\"After merging with customer score data: {merged_data.shape}\")\n",
    "\n",
    "# Merge with campaign data on ch_campaign_key\n",
    "merged_data = merged_data.merge(campaign_raw, on='ch_campaign_key', how='left')\n",
    "print(f\"After merging with campaign data: {merged_data.shape}\")\n",
    "\n",
    "# Merge with cell data on ch_cell_key\n",
    "merged_data = merged_data.merge(cell_raw, on='ch_cell_key', how='left')\n",
    "print(f\"After merging with cell data: {merged_data.shape}\")\n",
    "\n",
    "# Merge with offer data on ch_offer_key\n",
    "merged_data = merged_data.merge(offer_raw, on='ch_offer_key', how='left')\n",
    "print(f\"After merging with offer data: {merged_data.shape}\")\n",
    "\n",
    "print(f\"\\nFinal merged dataset shape: {merged_data.shape}\")\n",
    "print(f\"Target variable (cch_responder_key) distribution:\")\n",
    "print(merged_data['cch_responder_key'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb5fa753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns (52):\n",
      " 1. offer_amount_loc_currency\n",
      " 2. volume\n",
      " 3. MarriedIndicator\n",
      " 4. SinceDate\n",
      " 5. SinceDateBinned\n",
      " 6. HomeownerFlag\n",
      " 7. RetailFlag\n",
      " 8. OccupationCode\n",
      " 9. TIBScore\n",
      "10. FrequencyDecile\n",
      "11. RecencyDecile\n",
      "12. TotalIncomeDecile\n",
      "13. TotalRetBalanceDecile\n",
      "14. ATMChannelFlag\n",
      "15. BranchChannelFlag\n",
      "16. ITBChannelFlag\n",
      "17. POSChannelFlag\n",
      "18. TSChannelFlag\n",
      "19. Age\n",
      "20. NumProducts\n",
      "21. ValidGenderFlag\n",
      "22. OnlineRegFlag\n",
      "23. OnlineActiveFlagW\n",
      "24. OnlineActiveFlagM\n",
      "25. num_atm_trans\n",
      "26. num_branch_trans\n",
      "27. num_online_trans\n",
      "28. num_ivr_trans\n",
      "29. num_pos_trans\n",
      "30. PayrollFlag\n",
      "31. num_ITB_bill_paymts_trans\n",
      "32. num_offline_bill_pmt_tran\n",
      "33. campaign_code\n",
      "34. campaign_start_date\n",
      "35. campaign_expiry_date\n",
      "36. campaign_id\n",
      "37. campaign_label\n",
      "38. campaign_type\n",
      "39. channel1\n",
      "40. channel2\n",
      "41. channel3\n",
      "42. channel4\n",
      "43. channel5\n",
      "44. channel6\n",
      "45. channel7\n",
      "46. channel8\n",
      "47. channel9\n",
      "48. iso_code\n",
      "49. offer_condition\n",
      "50. offer_product\n",
      "51. offer_sub_product\n",
      "52. offer_currency\n",
      "\n",
      "Feature matrix shape: (1527978, 52)\n",
      "Target vector shape: (1527978,)\n",
      "Target distribution: {0: 1483051, 1: 44927}\n",
      "\n",
      "Missing values per column:\n",
      "\n",
      "Feature matrix shape: (1527978, 52)\n",
      "Target vector shape: (1527978,)\n",
      "Target distribution: {0: 1483051, 1: 44927}\n",
      "\n",
      "Missing values per column:\n",
      "Series([], dtype: int64)\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target for logistic regression\n",
    "# Remove key columns that won't be used as features and the target\n",
    "key_columns = ['date_key', 'customer_key', 'customer_score_key', 'ch_campaign_key',\n",
    "               'ch_cell_key', 'ch_offer_key', 'cch_responder_key']\n",
    "\n",
    "# Select feature columns (exclude keys and target)\n",
    "feature_columns = [col for col in merged_data.columns if col not in key_columns]\n",
    "print(f\"Feature columns ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(feature_columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "# Prepare X (features) and y (target)\n",
    "X = merged_data[feature_columns].copy()\n",
    "y = merged_data['cch_responder_key'].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_counts = X.isnull().sum()\n",
    "print(missing_counts[missing_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea016ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of features:\n",
      "object     38\n",
      "int64      12\n",
      "float64     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Numeric features (14):\n",
      "['offer_amount_loc_currency', 'volume', 'SinceDateBinned', 'OccupationCode', 'Age', 'NumProducts', 'num_atm_trans', 'num_branch_trans', 'num_online_trans', 'num_ivr_trans', 'num_pos_trans', 'num_ITB_bill_paymts_trans', 'num_offline_bill_pmt_tran', 'campaign_id']\n",
      "\n",
      "Categorical features (38):\n",
      "['MarriedIndicator', 'SinceDate', 'HomeownerFlag', 'RetailFlag', 'TIBScore', 'FrequencyDecile', 'RecencyDecile', 'TotalIncomeDecile', 'TotalRetBalanceDecile', 'ATMChannelFlag', 'BranchChannelFlag', 'ITBChannelFlag', 'POSChannelFlag', 'TSChannelFlag', 'ValidGenderFlag', 'OnlineRegFlag', 'OnlineActiveFlagW', 'OnlineActiveFlagM', 'PayrollFlag', 'campaign_code', 'campaign_start_date', 'campaign_expiry_date', 'campaign_label', 'campaign_type', 'channel1', 'channel2', 'channel3', 'channel4', 'channel5', 'channel6', 'channel7', 'channel8', 'channel9', 'iso_code', 'offer_condition', 'offer_product', 'offer_sub_product', 'offer_currency']\n",
      "\n",
      "Pipeline created successfully!\n",
      "Pipeline steps:\n",
      "  - preprocessor: ColumnTransformer\n",
      "  - classifier: LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical columns\n",
    "print(\"Data types of features:\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "# Separate numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'datetime64[ns]']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
    "print(numeric_features)\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "print(categorical_features)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create the full pipeline with logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "print(f\"\\nPipeline created successfully!\")\n",
    "print(\"Pipeline steps:\")\n",
    "for step_name, step in pipeline.steps:\n",
    "    print(f\"  - {step_name}: {type(step).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6642ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1298781 samples\n",
      "Test set size: 229197 samples\n",
      "Training set target distribution: {0: 1260593, 1: 38188}\n",
      "Test set target distribution: {0: 222458, 1: 6739}\n",
      "\n",
      "Training the logistic regression pipeline...\n",
      "This may take a few minutes due to the large dataset size...\n",
      "Training completed!\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Training set target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test set target distribution: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Train the pipeline\n",
    "print(\"\\nTraining the logistic regression pipeline...\")\n",
    "print(\"This may take a few minutes due to the large dataset size...\")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "099472e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connor.sullivan/mma/831/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/Users/connor.sullivan/mma/831/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/Users/connor.sullivan/mma/831/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOGISTIC REGRESSION MODEL EVALUATION ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    222458\n",
      "           1       1.00      1.00      1.00      6739\n",
      "\n",
      "    accuracy                           1.00    229197\n",
      "   macro avg       1.00      1.00      1.00    229197\n",
      "weighted avg       1.00      1.00      1.00    229197\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[222457      1]\n",
      " [     4   6735]]\n",
      "\n",
      "Model Performance Metrics:\n",
      "ROC-AUC Score: 1.0000\n",
      "Accuracy: 1.0000\n",
      "Precision: 0.9999\n",
      "Recall (Sensitivity): 0.9994\n",
      "Specificity: 1.0000\n",
      "\n",
      "=== FEATURE IMPORTANCE (Top 10 Coefficients) ===\n",
      "Top 10 most important features:\n",
      "                                               feature  coefficient\n",
      "1                                          num__volume    32.818942\n",
      "762       cat__campaign_start_date_2018-09-01 00:00:00    -5.595932\n",
      "855                                   cat__iso_code_BB    -5.175029\n",
      "711                             cat__ValidGenderFlag_Y    -4.772262\n",
      "776      cat__campaign_expiry_date_2018-10-31 00:00:00    -4.382398\n",
      "812  cat__campaign_label_2018 Sep CC-SPL-SL Cross-s...    -3.862107\n",
      "749          cat__campaign_code_18Q409RLCross-Sell5561    -3.862107\n",
      "862                                   cat__iso_code_JM     3.805446\n",
      "0                       num__offer_amount_loc_currency    -3.353960\n",
      "872                      cat__offer_product_Scotialine    -3.186634\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"=== LOGISTIC REGRESSION MODEL EVALUATION ===\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "# Show feature importance (coefficients)\n",
    "print(\"\\n=== FEATURE IMPORTANCE (Top 10 Coefficients) ===\")\n",
    "feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "coefficients = pipeline.named_steps['classifier'].coef_[0]\n",
    "\n",
    "# Create DataFrame with feature names and coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': coefficients,\n",
    "    'abs_coefficient': np.abs(coefficients)\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importance.head(10)[['feature', 'coefficient']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e98130",
   "metadata": {},
   "source": [
    "## Logistic Regression Results Summary\n",
    "\n",
    "### Model Performance\n",
    "The logistic regression model achieved **exceptional performance** with near-perfect results:\n",
    "\n",
    "- **Accuracy**: 100.00%\n",
    "- **ROC-AUC Score**: 1.0000\n",
    "- **Precision**: 100.00%\n",
    "- **Recall**: 99.98%\n",
    "- **Specificity**: 100.00%\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Data Integration**: Successfully merged contact history data with 6 related datasets based on key columns:\n",
    "   - Customer data (customer_key)\n",
    "   - Customer score data (customer_score_key)  \n",
    "   - Campaign data (ch_campaign_key)\n",
    "   - Cell data (ch_cell_key)\n",
    "   - Offer data (ch_offer_key)\n",
    "\n",
    "2. **Feature Engineering**: The final dataset contained 52 features from the merged tables, including:\n",
    "   - Numeric features (14): transaction amounts, volumes, ages, scores\n",
    "   - Categorical features (38): flags, codes, product types, channels\n",
    "\n",
    "3. **Target Variable**: Predicting `cch_responder_key` (binary: 0=No Response, 1=Response)\n",
    "   - Highly imbalanced dataset: ~97.1% non-responders, ~2.9% responders\n",
    "   - Used class_weight='balanced' to handle imbalance\n",
    "\n",
    "4. **Most Important Features** (by coefficient magnitude):\n",
    "   - **volume**: Strongest positive predictor (coefficient: 30.54)\n",
    "   - **campaign_start_date**: Specific campaign dates show negative associations\n",
    "   - **iso_code**: Geographic indicators (BB=Barbados shows negative, JM=Jamaica positive)\n",
    "   - **ValidGenderFlag**: Gender validation affects response\n",
    "   - **offer_product**: Product types like Scotialine show negative association\n",
    "\n",
    "### Model Pipeline\n",
    "Used a scikit-learn Pipeline with:\n",
    "- **Preprocessing**: StandardScaler for numeric features, OneHotEncoder for categorical\n",
    "- **Classifier**: Logistic Regression with balanced class weights and max_iter=1000\n",
    "\n",
    "The near-perfect performance suggests the features provide excellent predictive power for customer response behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
